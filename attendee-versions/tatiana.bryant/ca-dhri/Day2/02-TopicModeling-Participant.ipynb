{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fa41d1-2b68-4444-b6da-e7a888dc927e",
   "metadata": {},
   "source": [
    "# CaliDHRI Day 2: Afternoon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f04d42-d64a-4f00-81b6-9a3073443a47",
   "metadata": {},
   "source": [
    "## Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf03a1-8418-4f58-b3a7-74a26eba7b1d",
   "metadata": {},
   "source": [
    "Topic modeling is a popular approach to \"distant reading\" a corpus. It is a kind of unsupervised machine learning. Unlike in supervised methods where the human provides labeled training data from which the machine learns, in unsupervised machine learning, the machine learns from unlabeled data. There are benefits and drawbacks to both approaches. Topic modeling is a useful tool for understanding the subject matter of a dataset -- sometimes one that is too large for close reading or as a supplement to close reading.\n",
    "\n",
    "We will be doing LDA topic modeling. LDA stands for latent Dirichlet allocation. Don't get hung up on the terminology! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf5c91-472b-4abe-9915-29635e281613",
   "metadata": {},
   "source": [
    "### Topics are probabilities of aboutness\n",
    "\n",
    "Here's how the basic logic of LDA topic modeling works:\n",
    "* Start with a corpus of documents with any pre-processing/cleaning done.\n",
    "* Represent each document as a bag-of-words. In the BOW model, word order is lost and it just matters that the word occured within the bag (document).\n",
    "* The BOWs are represented as a term-document matrix consisting of columns and rows for each word's occurance.\n",
    "* The term-document matrices are fed to the algorithm. Each word in each document is compared (Is Word1 in Bag1? Yes. Is Word1 in Bag2? Yes. Is Word1 in Bag3? No. Is Word2 in Bag1? No. Is Word2 in Bag2? Yes. Is Word2 in Bag3? No. And so on.)\n",
    "* The computer learns from this process which words are likely to occur within the same document. The logic of LDA is that words that co-occur within a document are likely about the same thing.\n",
    "* The algorithm generates topics, which are probablistic groupings of words that are likely to co-occur within the corpus. \n",
    "* The algorithm does not tell you want the topics are about and it does not name the topics. That's up to the human!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b03c56-1229-4f95-8569-b1202dfe2198",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "In this notebook, we'll be using several libraries to prepare text from The Liberator for the algorithm, create topic models, and visualize the topics:\n",
    "* NLTK: data preparation\n",
    "* Gensim: topic modeling\n",
    "* Altair: visualization\n",
    "* ipywidgets: visualization\n",
    "* Matplotlib: visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5f7b1-1458-4aea-974f-cab0a4d511b4",
   "metadata": {},
   "source": [
    "These are already installed on JupyterHub, but if you were working locally, you'd want to make sure you installed them before trying to use them in your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedbdb1-851c-4760-866b-ce9b2f158d14",
   "metadata": {},
   "source": [
    "## The data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81f2da-97ea-4a6a-945c-b7939a697151",
   "metadata": {},
   "source": [
    "Here are the modules we'll use to load our data into the notebook:\n",
    "* Glob\n",
    "\n",
    "The import statements below will make them available for us to use in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90e1cc9-d6eb-468f-ab01-dc99a8e55317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340e61b-bdca-4c02-b27b-245e1b010ad9",
   "metadata": {},
   "source": [
    "Now we need to direct the notebook to the 187 files of the Liberator we'll be topic modeling. Each file is an issue of the Liberator. We haven't done any cleaning of the OCR for these files. \n",
    "\n",
    "Learn more about the Liberator in [these slides](https://docs.google.com/presentation/d/1gwAISOnn3Evq_LkX_0vhsYy0iTv7s5GHLUEGVKzN-tc/edit?usp=sharing). \n",
    "\n",
    "First we create a variable called directory_path, and we set that to the directory name where our files are stored. Then we can use glob, a handy module to navigate a directory structure, to set all of the files under directory_path to another variable called text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c08fc6-6602-496f-b0a4-8903fd2912e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'libSeparate'\n",
    "text_files = glob.glob(f\"{directory_path}/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f1341-279e-4719-af28-5c6c5ac1bf24",
   "metadata": {},
   "source": [
    "Let's check to see that the files were brought in. We can use the list index to call the first text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d26519d-b2cb-46ec-b327-d15d7b6bb75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'libSeparate/1912-08-02.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the number in the brackets to see a different file\n",
    "text_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25269d76-f449-4763-b877-9daa0d0e834d",
   "metadata": {},
   "source": [
    "The next step is to take those text files and make them into a Python list. First we create an empty list, listOfLib, and using a for loop, we iterate through the files in text_files, opening and reading them, and then apending them to listOfLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4d8452-983c-460d-a296-2ebf6e6b3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfLib = []\n",
    "for i in text_files:\n",
    "    try:\n",
    "        with open(i) as f:\n",
    "            listOfLib.append(f.read())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40290eb1-2980-4bc2-91d4-ffb79543f0e9",
   "metadata": {},
   "source": [
    "Let's check to make sure we got all the files. We can use the notation len() to see the length of both text_files and listOfLib are. The length is the number of items in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272a4e76-230d-4b32-8f64-58d98a90561b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755ebf9f-2684-4737-814e-e98e9e8b4fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listOfLib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a0d43-0d74-48a0-a504-d5d0e03accde",
   "metadata": {},
   "source": [
    "With the data now stored in our listOfLib, we can also call each of the items in the list using the notation below. The first brackets refer to a specific item in our list, in this case the 10th item. (Python starts counting at 0.) The second brackets are optional, and they are restricting what is returned to a slice of the data--characters 0 to 300. If we didn't slice, then we'd return the text of one entire issue of the Liberator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9df2ad0-cdcb-42ef-8e1f-d03211085154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE LIBERATOR \\n\\nA Weekly Newspaper Devoted to the Cause of Good Government and the Advancement of the American Negro. \\nVol. X 5 cts. a copy LOS ANGELES, CAL., MAY 3, 1912 $1.50 a year No. 24 \\n\\n\\nDr. James E. Shepard Greeted by a Great \\nOutpouring: of the People \\n\\n\\n\\nDr. James E. Shepard \\n\\nWesley Chape'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfLib[9][0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67172d4-2a3f-4891-9851-765b348e9cdb",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69a90d-c722-4429-a66e-edeac8392b78",
   "metadata": {},
   "source": [
    "Once the data is loaded and in our listOfLib, then we can move to preparing it for analysis. We are going to remove the stopwords, tokenize, and lemmatize the text. For this we will utilize NLTK's built-in stopwords, tokenizer, and lemmatizer. \n",
    "\n",
    "We import the NLTK modules below, plus a module called punctuation that will allow us to remove punctuation from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a31793e-38cf-4553-bed4-f7227ca2db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075760f-1c8c-4c7c-bd7d-82a061e81ede",
   "metadata": {},
   "source": [
    "Let's check out the NLTK stopword list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bf58c85-d7e2-4ee7-9e6f-2757ed69e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "standardStop = stopwords.words('english')\n",
    "print (standardStop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c8902-358e-4ab3-9909-ce2f27b0d00b",
   "metadata": {},
   "source": [
    "The standard stopwords may not be sufficient for the text we will analysis from the Liberator. We can add stopwords by creating a list called extraStop that we will combine with the standard list in a new list. The new list will be called myStopWords, and it combines the punctuation marks, standardStop, and extraStop lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e868cad5-37d8-48a7-a171-803b5659d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraStop = ['``',\"''\",\"'re\",\"'s\",\"'re\",'``',\"''\",\"'ll\",\"--\",\"\\'\\'\",\"...\", \"n\\'t\",'one','would','use','subject','from',\n",
    "             \"\\'m\",\"\\'ve\", \"los\", \"angeles\", \"liberator\", \"phone\", \"street\", \"mrs.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f2577f-e38c-4e3d-852c-4b6364ab70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myStopWords = list(punctuation) + standardStop + extraStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f254b75-b3db-4e24-8168-e9ee93ecb2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '``', \"''\", \"'re\", \"'s\", \"'re\", '``', \"''\", \"'ll\", '--', \"''\", '...', \"n't\", 'one', 'would', 'use', 'subject', 'from', \"'m\", \"'ve\", 'los', 'angeles', 'liberator', 'phone', 'street', 'mrs.']\n"
     ]
    }
   ],
   "source": [
    "print(myStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6a26f-ac24-4b70-a001-d880cc364b92",
   "metadata": {},
   "source": [
    "Now we'll create a function to tokenize and lemmatize the text. As you'll remember from this morning, lemmatization groups together forms of a word.  \n",
    "\n",
    "Let's take note of all the preprocessing happening in this function:\n",
    "\n",
    "* We create 2 empty lists called listOfLibWords and allLibWords\n",
    "* Then, in a for loop, we go through each of the Liberators stored in our listOfLib and...\n",
    "    * For ever word, we lowercase and tokenize it if it is not on myStopWords and it has more than 3 letters\n",
    "    * The tokens are stored in a temporary list called processedText\n",
    "    * We append the processedText list to the listOfLibWords\n",
    "* And in another for loop, we go through each token in processedText and add it to the allLibWords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b933c81-c5aa-4b69-ba2b-94f51613834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfLibWords = []\n",
    "allLibWords = []\n",
    "for i in listOfLib:\n",
    "    processedText = [w for w in word_tokenize(i.lower()) if w not in myStopWords and len(w) > 3]\n",
    "    listOfLibWords.append(processedText)\n",
    "    for token in processedText:\n",
    "        allLibWords.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0705d-0bc0-46bf-8e14-0b98272df385",
   "metadata": {},
   "source": [
    "Then we need to lemmatize the tokens in allLibWords. We'll create yet another empty list called listOfLemWords. We'll name a variable wordnet_lemmatizer and set it to NLTK's WordNetLemmatizer. Then in another for loop, we'll iterate through the tokens in listOfLibWords and add them to listOfLemWords once they have been lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c1d1150-4d4c-4186-a028-a8f5afc27314",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfLemWords = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for i in listOfLibWords:\n",
    "    listOfLemWords.append([wordnet_lemmatizer.lemmatize(w) for w in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7fb61-97e8-4bed-9049-d214c0b81858",
   "metadata": {},
   "source": [
    "Let's take a look at a slice of tokens from list item 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "360c4ab7-71e0-4e63-b296-cdaef5a88926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weekly',\n",
       " 'newspaper',\n",
       " 'devoted',\n",
       " 'cause',\n",
       " 'good',\n",
       " 'government',\n",
       " 'advancement',\n",
       " 'american',\n",
       " 'negro',\n",
       " 'copy',\n",
       " 'cal.',\n",
       " 'oct.',\n",
       " '1912',\n",
       " '1.50',\n",
       " 'year',\n",
       " 'door',\n",
       " 'hope',\n",
       " 'closed',\n",
       " 'negro',\n",
       " 'progress',\n",
       " 'sive',\n",
       " 'party',\n",
       " 'thoughtful',\n",
       " 'negro',\n",
       " 'reason']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfLemWords[6][0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee9bd4-1fab-499d-91eb-1db50974a238",
   "metadata": {},
   "source": [
    "## Create the bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cfc148-e1d9-4fc2-a6b7-756d6e95ede1",
   "metadata": {},
   "source": [
    "Gensim expects the data in a certain format in order to do topic modeling. First, we need to create a Python dictionary from our list of words. Then we need to translate the dictionary into a bag-of-words. \n",
    "\n",
    "But before any of that, we need to import Gensim and the corpora and models modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2efff976-530c-44b7-8753-f57fd7d9045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc0b44-a4c7-4d8b-a1ff-9b97e3c26175",
   "metadata": {},
   "source": [
    "Now we can define a variable called dictionary. Gensim's corpora module will map the words in each document to integer IDs, in other words, assigning each unique word in the corpus a corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b53c04fc-5422-428d-bcf3-578bd84715c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(listOfLemWords) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dac33-28df-4260-b4e2-19625bc119c4",
   "metadata": {},
   "source": [
    "We can check the ID of any token an our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4ddb5cb-007b-446d-987d-c544f1773466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "#try looking up another word to find its ID\n",
    "print(dictionary.token2id['broadway'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa92837-9127-4293-b24f-7d14bf4f8fb0",
   "metadata": {},
   "source": [
    "Then, we define a new variable called corpus. Gensim has a module called doc2bow that will convert a document into the bag-of-words format. The BOW will be a list of tuples -- consisting of word ID and frequency pairs -- that represent each document in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a0503ae-a813-43b9-b6dc-f04f536ac3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in listOfLemWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac68f5e6-ee97-472b-bde7-9b51bc99c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 2), (4, 1), (5, 1), (7, 1), (9, 1), (11, 5), (12, 1), (13, 1), (15, 1), (18, 1), (24, 1), (28, 1), (29, 1), (30, 3), (31, 2), (33, 1), (34, 2), (35, 1), (38, 1), (39, 1), (45, 1), (46, 1), (47, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (62, 1), (73, 1), (74, 1), (75, 3), (80, 1), (81, 1), (85, 3), (87, 2), (88, 1), (91, 1), (94, 1), (95, 4), (96, 1), (98, 1), (101, 1), (102, 1), (103, 5), (104, 1), (105, 3), (108, 6), (109, 2), (116, 2), (118, 1), (119, 1), (124, 1), (131, 1), (133, 1), (134, 1), (148, 1), (151, 1), (155, 2), (158, 1), (160, 5), (162, 4), (164, 1), (166, 3), (167, 1), (173, 2), (176, 1), (177, 1), (180, 1), (182, 4), (183, 2), (186, 1), (192, 3), (193, 1), (195, 3), (196, 3), (197, 1), (199, 7), (200, 1), (201, 1), (203, 2), (205, 1), (212, 1), (213, 12), (214, 1), (216, 3), (218, 3), (223, 5), (228, 1), (231, 1), (232, 3), (234, 3), (235, 5), (241, 2), (244, 2), (247, 1), (250, 3), (252, 5), (259, 1), (261, 2), (262, 1), (263, 1), (267, 2), (268, 2), (270, 5), (272, 4), (274, 8), (275, 8), (277, 1), (279, 1), (281, 5), (282, 1), (286, 1), (287, 1), (290, 3), (291, 1), (292, 1), (294, 5), (296, 15), (299, 1), (300, 2), (303, 1), (305, 1), (306, 4), (310, 1), (311, 1), (315, 1), (316, 1), (323, 1), (325, 1), (328, 2), (331, 2), (334, 3), (336, 2), (338, 7), (339, 8), (342, 3), (345, 3), (347, 3), (350, 1), (351, 1), (354, 1), (359, 1), (360, 2), (361, 1), (362, 2), (365, 2), (367, 1), (368, 1), (369, 1), (371, 2), (378, 1), (379, 2), (385, 1), (387, 1), (397, 2), (400, 1), (402, 1), (408, 1), (410, 1), (415, 1), (417, 1), (421, 1), (424, 1), (425, 2), (427, 1), (431, 1), (433, 1), (438, 6), (439, 1), (442, 3), (444, 2), (446, 1), (448, 1), (449, 2), (456, 1), (462, 1), (464, 1), (473, 1), (474, 1), (476, 1), (481, 1), (482, 1), (485, 7), (488, 1), (490, 11), (492, 5), (493, 3), (494, 2), (495, 3), (501, 1), (503, 2), (508, 1), (509, 1), (510, 3), (511, 2), (512, 1), (517, 1), (521, 1), (525, 1), (533, 1), (535, 1), (536, 2), (541, 6), (542, 2), (544, 1), (553, 4), (554, 1), (556, 1), (559, 1), (563, 1), (564, 2), (565, 1), (566, 2), (570, 6), (574, 3), (575, 2), (576, 2), (577, 2), (578, 4), (583, 2), (584, 4), (592, 2), (593, 1), (594, 7), (597, 2), (599, 1), (600, 2), (601, 10), (604, 1), (610, 6), (613, 5), (614, 2), (618, 3), (620, 1), (621, 1), (625, 1), (627, 8), (628, 1), (633, 1), (634, 1), (638, 2), (639, 2), (640, 1), (641, 4), (643, 1), (645, 1), (646, 4), (647, 4), (649, 1), (650, 1), (653, 1), (654, 2), (655, 1), (656, 1), (657, 2), (658, 2), (661, 11), (662, 1), (664, 5), (666, 1), (667, 2), (668, 3), (669, 4), (673, 1), (675, 2), (677, 1), (685, 1), (686, 1), (687, 1), (694, 3), (695, 1), (700, 4), (707, 1), (710, 1), (715, 1), (716, 3), (720, 1), (723, 1), (726, 6), (730, 1), (731, 2), (733, 2), (736, 1), (737, 3), (739, 1), (740, 2), (744, 3), (746, 2), (750, 4), (753, 2), (754, 6), (756, 1), (759, 1), (765, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[9][0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4a716-eff1-4ba9-9ea2-3d01869fb407",
   "metadata": {},
   "source": [
    "## Run the model and view topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51d3e8-fb4b-4a9b-ac5c-21950fed08e0",
   "metadata": {},
   "source": [
    "Gensim has a module called models for LDA topic modeling. We'll import the models module and then create our topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01c850f7-bf02-49b0-8d97-aec8277d06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19ab65-7f40-41b5-ae05-5b1b79ca7a03",
   "metadata": {},
   "source": [
    "There are parameters that we have the option to set when creating the topic model. We're using just couple here:\n",
    "\n",
    "* num_topics: the number of topics our model will create. This is a parameter you can adjust based on your corpus size and the length of the documents. We are starting with 10 topics.\n",
    "* passes: the number of times the algorithm will pass through the documents. A higher number is likely to yield higher quality topics, but will take more time to run. \n",
    "\n",
    "You can find the full list of available parameters on this webpage: https://radimrehurek.com/gensim/models/word2vec.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "938f97d3-d654-4060-b768-da5e69f2f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics=10, \n",
    "                                           id2word = dictionary, \n",
    "                                           passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c6bf-d566-4345-a2d6-0327740f5c76",
   "metadata": {},
   "source": [
    "The notation show_topics() will return the 10 most salient words in each of our topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "295051f4-5a42-46d9-8d57-910b67cb465c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"spear\" + 0.003*\"426-428\" + 0.003*\"joseph\" + 0.003*\"1345.\" + 0.002*\"sander\" + 0.002*\"spring\" + 0.002*\"president\" + 0.002*\"morton\" + 0.002*\"price\" + 0.002*\"1345\"'),\n",
       " (1,\n",
       "  '0.000*\"city\" + 0.000*\"negro\" + 0.000*\"colored\" + 0.000*\"race\" + 0.000*\"people\" + 0.000*\"white\" + 0.000*\"state\" + 0.000*\"year\" + 0.000*\"home\" + 0.000*\"south\"'),\n",
       " (2,\n",
       "  '0.009*\"main\" + 0.008*\"negro\" + 0.006*\"colored\" + 0.005*\"city\" + 0.005*\"home\" + 0.005*\"office\" + 0.004*\"people\" + 0.004*\"good\" + 0.004*\"broadway\" + 0.003*\"every\"'),\n",
       " (3,\n",
       "  '0.002*\"barley\" + 0.001*\"cuban\" + 0.001*\"chamber\" + 0.001*\"kellum\" + 0.001*\"bean\" + 0.001*\"pork\" + 0.001*\"depravity\" + 0.001*\"pimp\" + 0.001*\"harvested\" + 0.001*\"goat\"'),\n",
       " (4,\n",
       "  '0.001*\"wickliffe\" + 0.001*\"spelman\" + 0.001*\"mortgage\" + 0.001*\"herald\" + 0.001*\"insula\" + 0.001*\"bastard\" + 0.001*\"lieves\" + 0.001*\"rockefeller\" + 0.001*\"seminary\" + 0.001*\"reverend\"'),\n",
       " (5,\n",
       "  '0.005*\"city\" + 0.003*\"pasadena\" + 0.002*\"made\" + 0.002*\"business\" + 0.002*\"texas\" + 0.002*\"place\" + 0.002*\"make\" + 0.002*\"well\" + 0.002*\"ford\" + 0.002*\"cure\"'),\n",
       " (6,\n",
       "  '0.009*\"negro\" + 0.007*\"white\" + 0.006*\"colored\" + 0.005*\"city\" + 0.005*\"race\" + 0.005*\"people\" + 0.005*\"good\" + 0.004*\"south\" + 0.004*\"every\" + 0.004*\"home\"'),\n",
       " (7,\n",
       "  '0.007*\"church\" + 0.006*\"main\" + 0.004*\"home\" + 0.003*\"city\" + 0.003*\"bishop\" + 0.003*\"good\" + 0.003*\"silver\" + 0.003*\"broadway\" + 0.003*\"negro\" + 0.003*\"colored\"'),\n",
       " (8,\n",
       "  '0.000*\"negro\" + 0.000*\"white\" + 0.000*\"colored\" + 0.000*\"main\" + 0.000*\"woman\" + 0.000*\"good\" + 0.000*\"people\" + 0.000*\"office\" + 0.000*\"south\" + 0.000*\"city\"'),\n",
       " (9,\n",
       "  '0.005*\"colored\" + 0.005*\"people\" + 0.004*\"race\" + 0.004*\"white\" + 0.004*\"miss\" + 0.003*\"pasadena\" + 0.003*\"good\" + 0.003*\"home\" + 0.003*\"great\" + 0.003*\"south\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()\n",
    "#try adding a topic number (e.g., 5) and topn= 20 to see twenty words in topic 5, for example show_topics(9, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04649de9-6d69-42c4-aa96-29daf0a8c2e2",
   "metadata": {},
   "source": [
    "The code below will print a cleaner view of the top 20 words in our topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "709f9270-0ebc-4ec8-a3d3-45bbb1bb7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "spear\n",
      "426-428\n",
      "joseph\n",
      "1345.\n",
      "sander\n",
      "spring\n",
      "president\n",
      "morton\n",
      "price\n",
      "1345\n",
      "thompson\n",
      "mingo\n",
      "assassin\n",
      "tilman\n",
      "good\n",
      "hand\n",
      "brush\n",
      "mckinley\n",
      "guilty\n",
      "save\n",
      "\n",
      "\n",
      "Topic 1\n",
      "city\n",
      "negro\n",
      "colored\n",
      "race\n",
      "people\n",
      "white\n",
      "state\n",
      "year\n",
      "home\n",
      "south\n",
      "business\n",
      "main\n",
      "church\n",
      "good\n",
      "every\n",
      "work\n",
      "woman\n",
      "money\n",
      "made\n",
      "office\n",
      "\n",
      "\n",
      "Topic 2\n",
      "main\n",
      "negro\n",
      "colored\n",
      "city\n",
      "home\n",
      "office\n",
      "people\n",
      "good\n",
      "broadway\n",
      "every\n",
      "state\n",
      "church\n",
      "race\n",
      "made\n",
      "white\n",
      "first\n",
      "money\n",
      "year\n",
      "business\n",
      "work\n",
      "\n",
      "\n",
      "Topic 3\n",
      "barley\n",
      "cuban\n",
      "chamber\n",
      "kellum\n",
      "bean\n",
      "pork\n",
      "depravity\n",
      "pimp\n",
      "harvested\n",
      "goat\n",
      "cham¬\n",
      "imperial\n",
      "revenue\n",
      "3,500,000\n",
      "bride-elect\n",
      "alfalfa\n",
      "americanized\n",
      "sanitation\n",
      "spiel\n",
      "sheep\n",
      "\n",
      "\n",
      "Topic 4\n",
      "wickliffe\n",
      "spelman\n",
      "mortgage\n",
      "herald\n",
      "insula\n",
      "bastard\n",
      "lieves\n",
      "rockefeller\n",
      "seminary\n",
      "reverend\n",
      "appropriate\n",
      "rochelle\n",
      "garret\n",
      "seashore\n",
      "term¬\n",
      "browne\n",
      "modoc\n",
      "shoemaker\n",
      "krown\n",
      "rotting\n",
      "\n",
      "\n",
      "Topic 5\n",
      "city\n",
      "pasadena\n",
      "made\n",
      "business\n",
      "texas\n",
      "place\n",
      "make\n",
      "well\n",
      "ford\n",
      "cure\n",
      "name\n",
      "property\n",
      "time\n",
      "south\n",
      "smith\n",
      "club\n",
      "remedy\n",
      "state\n",
      "hall\n",
      "president\n",
      "\n",
      "\n",
      "Topic 6\n",
      "negro\n",
      "white\n",
      "colored\n",
      "city\n",
      "race\n",
      "people\n",
      "good\n",
      "south\n",
      "every\n",
      "home\n",
      "year\n",
      "state\n",
      "office\n",
      "pasadena\n",
      "time\n",
      "business\n",
      "work\n",
      "woman\n",
      "main\n",
      "make\n",
      "\n",
      "\n",
      "Topic 7\n",
      "church\n",
      "main\n",
      "home\n",
      "city\n",
      "bishop\n",
      "good\n",
      "silver\n",
      "broadway\n",
      "negro\n",
      "colored\n",
      "valley\n",
      "smith\n",
      "general\n",
      "made\n",
      "place\n",
      "year\n",
      "minister\n",
      "cotton\n",
      "every\n",
      "town\n",
      "\n",
      "\n",
      "Topic 8\n",
      "negro\n",
      "white\n",
      "colored\n",
      "main\n",
      "woman\n",
      "good\n",
      "people\n",
      "office\n",
      "south\n",
      "city\n",
      "pasadena\n",
      "home\n",
      "every\n",
      "year\n",
      "state\n",
      "work\n",
      "country\n",
      "church\n",
      "race\n",
      "broadway\n",
      "\n",
      "\n",
      "Topic 9\n",
      "colored\n",
      "people\n",
      "race\n",
      "white\n",
      "miss\n",
      "pasadena\n",
      "good\n",
      "home\n",
      "great\n",
      "south\n",
      "black\n",
      "negro\n",
      "church\n",
      "every\n",
      "make\n",
      "city\n",
      "washington\n",
      "business\n",
      "many\n",
      "work\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Topic '+str(i))\n",
    "    for j in lda_model.show_topic(i, topn=20):\n",
    "            print(j[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b71de4-161d-4720-a266-c8c1181fa589",
   "metadata": {},
   "source": [
    "## Visualize topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f1eec-682a-4b9f-8c95-9f786b8a27e7",
   "metadata": {},
   "source": [
    "Visualizing our topics will help us make sense of them. The code blocks below are a bit more complicated than what we saw above. \n",
    "\n",
    "First, let's import the modules we'll use in this section. \n",
    "\n",
    "* Altair: visualization library for Python\n",
    "* ipywidgets: will make our visualizations interactive\n",
    "* Pandas: common Python library for data science that is useful when working with tabular data\n",
    "* Matplotlib: a very popular Python visualization library\n",
    "* Seaborn: another Python visualization library \n",
    "\n",
    "Where you see the statements written \"import ______ as ____\", that's a way to abbreviate a module's name, mostly for the convenience of not typing the full name later. The abbreviations used here are common, so you may seem them if you look at other code in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ab201bc-2714-4702-acf4-99c0fcf833b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import ipywidgets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b33c42-2b58-4492-8f59-14d443f592f2",
   "metadata": {},
   "source": [
    "pyLDAvis is another frequently used Python library for visualizing the outputs of Gensim topic models. The code is a bit simpler than what we're using here, but we think these custom visualizations are nicer! If you wanted to use pyLDAvis, you'd need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf875d0f-fc51-4c38-b7f0-91acc5436cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models\n",
    "#pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, id2word, mds=\"mmds\", R=30)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be971c-2f57-410e-b1ed-105c347a4601",
   "metadata": {},
   "source": [
    "Let's get back to our visualiations. First we want to create a list of just the tokens in the topics. \n",
    "\n",
    "Here's what's happening below:\n",
    "* Create a variable called topics and set it to show_topics\n",
    "* Create an empty list called topic_words\n",
    "* Iterate through topics and append the tokens to the topic_words list\n",
    "\n",
    "At the end, we print topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ce541a2-0115-4601-bfd4-9b82d8f03d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['city', 'pasadena', 'made', 'business', 'texas', 'place', 'make', 'well', 'ford', 'cure'], ['main', 'negro', 'colored', 'city', 'home', 'office', 'people', 'good', 'broadway', 'every'], ['negro', 'white', 'colored', 'city', 'race', 'people', 'good', 'south', 'every', 'home'], ['church', 'main', 'home', 'city', 'bishop', 'good', 'silver', 'broadway', 'negro', 'colored'], ['negro', 'white', 'colored', 'main', 'woman', 'good', 'people', 'office', 'south', 'city']]\n"
     ]
    }
   ],
   "source": [
    "topics=lda_model.show_topics(5, 10,formatted=False)\n",
    "topic_words = []\n",
    "for topic in topics:\n",
    "    topic_words.append([wd[0] for wd in topic[1]])\n",
    "print(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f08556-cf0a-4cfc-b443-a4c3bc021c96",
   "metadata": {},
   "source": [
    "Then, we want to create a Pandas dataframe from our corpus. Try not to worry to much about the syntax below, which mixes in Pandas syntax that we haven't yet seen. The important thing to know is that we are creating a dataframe, which you can think about like superpowered, code-accessible spreadsheet. Our dataframe will include the probability for each topic in each file in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c0cf4f2-7600-4df6-beaf-82f58f970f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 10\n",
    "dftopics = pd.DataFrame({'corpus':corpus})\n",
    "filenames = []\n",
    "topiclist = []\n",
    "topicprob = []\n",
    "topiccorpus = []\n",
    "for i,row in dftopics.iterrows():\n",
    "    probs = [0]*ntopics\n",
    "    for t in lda_model.get_document_topics(row['corpus']):\n",
    "        probs[t[0]] = t[1]\n",
    "    for j in range(ntopics):\n",
    "        topiccorpus.append('file'+str(i))\n",
    "        topiclist.append(j)\n",
    "        topicprob.append(probs[j])\n",
    "topic_dataframe = pd.DataFrame({'corpus':topiccorpus,'topic':topiclist,'prob':topicprob})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a71cc-59b3-4271-a905-cf72f9fb0980",
   "metadata": {},
   "source": [
    "If we call the dataframe, then we can get a feel for the tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbbf6966-0857-49d1-aa92-e990fe1bba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>file100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>file100</td>\n",
       "      <td>6</td>\n",
       "      <td>0.879236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>file100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.120555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>file100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>file100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       corpus  topic      prob\n",
       "0       file0      0  0.000000\n",
       "1       file0      1  0.000000\n",
       "2       file0      2  0.998753\n",
       "3       file0      3  0.000000\n",
       "4       file0      4  0.000000\n",
       "...       ...    ...       ...\n",
       "1005  file100      5  0.000000\n",
       "1006  file100      6  0.879236\n",
       "1007  file100      7  0.120555\n",
       "1008  file100      8  0.000000\n",
       "1009  file100      9  0.000000\n",
       "\n",
       "[1010 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09254a60-0d9e-4d93-bc12-7a6f1169b4c1",
   "metadata": {},
   "source": [
    "Next we can sort the dataframe so that it goes in ascending order by topic and probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c32706-2ddc-44d4-8899-b98a3e2e0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataframe = topic_dataframe.sort_values(by=['topic','prob'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1de10-561f-465a-803b-09aefb823e0a",
   "metadata": {},
   "source": [
    "The following code is a lot to digest, but it will take our dataframe and generate an interactive visualization showing the dispersion of topics within our issues of the Liberator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996fbda-91ef-419f-8c97-a20da35ae720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 10\n",
    "numwords = 10\n",
    "selection = alt.selection_single(on='mouseover',fields=['topic'], bind='legend')\n",
    "selopac = alt.selection_single(on='mouseover',fields=['topic'],bind='legend')\n",
    "def topicbars(Topic):\n",
    "    a = Topic\n",
    "    topics_df_sorted = topic_dataframe[topic_dataframe['topic']==a].sort_values(by=['topic','prob'],ascending=False)\n",
    "    topics_df_sorted = topics_df_sorted.append(topic_dataframe[topic_dataframe['topic']!=a])\n",
    "\n",
    "    x=lda_model.show_topics(num_topics=ntopics, num_words=numwords,formatted=False)\n",
    "    topic_words = []\n",
    "    for topic in x:\n",
    "        topic_words.append([wd[0] for wd in topic[1]])\n",
    "    topics_df_sorted['topicwords'] = ''\n",
    "    for i,row in topics_df_sorted.iterrows():\n",
    "        topics_df_sorted.loc[i,'topicwords'] = ', '.join(topic_words[row['topic']])\n",
    "    print(\"\\n\"+', '.join(topic_words[Topic])+\"\\n\")\n",
    "    \n",
    "    chart = alt.Chart(topics_df_sorted).mark_bar().encode(\n",
    "        x = 'sum(prob)',\n",
    "        y = alt.X('corpus',sort=None),\n",
    "        color = alt.Color('topic:N'),\n",
    "        order = alt.Order('corpus'),\n",
    "        opacity=alt.condition(selopac, alt.value(1), alt.value(0.5)),\n",
    "        tooltip=['corpus', 'topic', 'prob', 'topicwords']\n",
    "    ).add_selection(selopac)\n",
    "        \n",
    "    return chart\n",
    "\n",
    "ipywidgets.interact(topicbars,Topic=range(ntopics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e7041-81f2-4011-9689-c72c5de5b7bc",
   "metadata": {},
   "source": [
    "Then we can modify our dataframe so that topic is now rendered as a column. This will set us up for another visualization that will show how prevalent each topic is in each issue of the Liberator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dbea4-96db-4c05-a764-db186a5546b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataframe.pivot(index='corpus',columns='topic',values='prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb4b9a-b6a2-41d7-9558-087a49f1a26f",
   "metadata": {},
   "source": [
    "Let's integrate that code above into the code below to create a new dataframe where the words in each topic will now be brought in, instead of the topic number. Then we'll call the new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4c31b-73d1-4ea0-9c21-b2c910b88a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 10\n",
    "numwords = 10\n",
    "\n",
    "df_topic_words = topic_dataframe.pivot(index='corpus',columns='topic',values='prob')\n",
    "\n",
    "x=lda_model.show_topics(num_topics=ntopics, num_words=numwords,formatted=False)\n",
    "topic_words = []\n",
    "for topic in x:\n",
    "    topic_words.append(','.join([wd[0] for wd in topic[1]]))\n",
    "df_topic_words.columns = topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89431f52-add1-4837-8a04-63b271015d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c3be6-5088-428e-9425-39a365bc1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 80))\n",
    "ax = sns.heatmap(df_topic_words,\n",
    "                 cmap=\"YlGnBu\",\n",
    "                 annot=True)\n",
    "plt.tick_params(bottom=False, labelbottom=False, top=True, labeltop=True)\n",
    "plt.xticks(rotation=30, ha='left')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f991af1-df18-4a4a-ba51-4a29926e0453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
